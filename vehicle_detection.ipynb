{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d15eacc8-e24c-4070-af96-bc8f5bae9089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\PCAdmin\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l.pt to 'yolo11l.pt': 100% ━━━━━━━━━━━━ 49.0MB 958.2KB/s 52.4s52.4s<0.0ss\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the model\n",
    "model = YOLO(\"yolo11l.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6419aa7e-20a8-422d-8897-6d70d4f1274f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'person',\n",
       " 1: 'bicycle',\n",
       " 2: 'car',\n",
       " 3: 'motorcycle',\n",
       " 4: 'airplane',\n",
       " 5: 'bus',\n",
       " 6: 'train',\n",
       " 7: 'truck',\n",
       " 8: 'boat',\n",
       " 9: 'traffic light',\n",
       " 10: 'fire hydrant',\n",
       " 11: 'stop sign',\n",
       " 12: 'parking meter',\n",
       " 13: 'bench',\n",
       " 14: 'bird',\n",
       " 15: 'cat',\n",
       " 16: 'dog',\n",
       " 17: 'horse',\n",
       " 18: 'sheep',\n",
       " 19: 'cow',\n",
       " 20: 'elephant',\n",
       " 21: 'bear',\n",
       " 22: 'zebra',\n",
       " 23: 'giraffe',\n",
       " 24: 'backpack',\n",
       " 25: 'umbrella',\n",
       " 26: 'handbag',\n",
       " 27: 'tie',\n",
       " 28: 'suitcase',\n",
       " 29: 'frisbee',\n",
       " 30: 'skis',\n",
       " 31: 'snowboard',\n",
       " 32: 'sports ball',\n",
       " 33: 'kite',\n",
       " 34: 'baseball bat',\n",
       " 35: 'baseball glove',\n",
       " 36: 'skateboard',\n",
       " 37: 'surfboard',\n",
       " 38: 'tennis racket',\n",
       " 39: 'bottle',\n",
       " 40: 'wine glass',\n",
       " 41: 'cup',\n",
       " 42: 'fork',\n",
       " 43: 'knife',\n",
       " 44: 'spoon',\n",
       " 45: 'bowl',\n",
       " 46: 'banana',\n",
       " 47: 'apple',\n",
       " 48: 'sandwich',\n",
       " 49: 'orange',\n",
       " 50: 'broccoli',\n",
       " 51: 'carrot',\n",
       " 52: 'hot dog',\n",
       " 53: 'pizza',\n",
       " 54: 'donut',\n",
       " 55: 'cake',\n",
       " 56: 'chair',\n",
       " 57: 'couch',\n",
       " 58: 'potted plant',\n",
       " 59: 'bed',\n",
       " 60: 'dining table',\n",
       " 61: 'toilet',\n",
       " 62: 'tv',\n",
       " 63: 'laptop',\n",
       " 64: 'mouse',\n",
       " 65: 'remote',\n",
       " 66: 'keyboard',\n",
       " 67: 'cell phone',\n",
       " 68: 'microwave',\n",
       " 69: 'oven',\n",
       " 70: 'toaster',\n",
       " 71: 'sink',\n",
       " 72: 'refrigerator',\n",
       " 73: 'book',\n",
       " 74: 'clock',\n",
       " 75: 'vase',\n",
       " 76: 'scissors',\n",
       " 77: 'teddy bear',\n",
       " 78: 'hair drier',\n",
       " 79: 'toothbrush'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_list = model.names\n",
    "class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce9ff64d-ad55-4f1d-ba63-bb77b87d6d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"test_videos/clip.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89127e24-5569-40ce-acf3-b5199300c311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 190.0432,  410.2811,  725.4126,  663.1446],\n",
       "        [1097.3617,  277.6153, 1151.8217,  316.8832],\n",
       "        [ 730.4357,  207.3015,  800.1662,  265.4578],\n",
       "        [ 859.4677,  325.2623,  876.4031,  363.3753],\n",
       "        [1159.3859,  306.3623, 1229.4801,  349.4019],\n",
       "        [1209.2560,  285.4540, 1279.3794,  329.4029],\n",
       "        [ 749.4686,  337.1175,  770.9154,  371.5647],\n",
       "        [ 972.7072,  241.0021, 1001.0358,  270.0042],\n",
       "        [ 824.2639,  230.8312,  858.6873,  258.4812],\n",
       "        [ 887.2084,  209.4839,  910.4047,  231.7703],\n",
       "        [ 797.6534,  226.3748,  822.2659,  247.0495],\n",
       "        [ 789.7700,  322.0614,  808.4115,  356.7692],\n",
       "        [1132.3430,  266.0194, 1172.2306,  292.0472],\n",
       "        [ 830.0010,  314.1409,  845.2081,  344.7407],\n",
       "        [1047.3440,  264.3489, 1082.9696,  285.7319],\n",
       "        [1169.9878,  257.1059, 1211.9612,  285.8045],\n",
       "        [ 841.6600,  174.1239,  876.3717,  200.2803],\n",
       "        [ 871.2196,  233.0197,  905.7315,  270.1219],\n",
       "        [ 871.1706,  233.4162,  905.3557,  270.4720],\n",
       "        [ 792.0891,  338.3528,  815.9497,  359.7331],\n",
       "        [ 860.0303,  345.1631,  881.9920,  371.1869]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].boxes.xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "638d2bf8-e5fc-4aa4-b03a-a8004c42246a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "cls: tensor([5., 2., 2., 5., 3., 2., 2., 0., 3., 3., 3., 0., 2., 3., 2., 3., 0., 0., 2., 2., 2., 0., 5., 0., 3., 0., 0., 2.])\n",
       "conf: tensor([0.9267, 0.7147, 0.5643, 0.1277, 0.6758, 0.7405, 0.3049, 0.2275, 0.6145, 0.5932, 0.5926, 0.4706, 0.8113, 0.5208, 0.4179, 0.3570, 0.2508, 0.3630, 0.6014, 0.3318, 0.6304, 0.3709, 0.6210, 0.4704, 0.5640, 0.3782, 0.3917, 0.2943])\n",
       "data: tensor([[5.6978e-01, 4.5072e+02, 4.3946e+02, 6.7447e+02, 1.0000e+00, 9.2672e-01, 5.0000e+00],\n",
       "        [1.1140e+03, 2.8478e+02, 1.1734e+03, 3.2502e+02, 2.0000e+00, 7.1473e-01, 2.0000e+00],\n",
       "        [1.1535e+03, 2.7034e+02, 1.2052e+03, 3.0420e+02, 4.0000e+00, 5.6428e-01, 2.0000e+00],\n",
       "        [7.2602e+02, 2.0535e+02, 8.1791e+02, 2.8224e+02, 5.0000e+00, 1.2770e-01, 5.0000e+00],\n",
       "        [7.7767e+02, 4.5480e+02, 8.1152e+02, 5.0647e+02, 6.0000e+00, 6.7585e-01, 3.0000e+00],\n",
       "        [8.2469e+02, 2.4582e+02, 8.5891e+02, 2.7529e+02, 8.0000e+00, 7.4050e-01, 2.0000e+00],\n",
       "        [8.7495e+02, 2.1904e+02, 9.0192e+02, 2.4471e+02, 9.0000e+00, 3.0494e-01, 2.0000e+00],\n",
       "        [8.5002e+02, 3.2191e+02, 8.6969e+02, 3.5900e+02, 2.0000e+01, 2.2750e-01, 0.0000e+00],\n",
       "        [8.7628e+02, 3.8938e+02, 8.9982e+02, 4.2794e+02, 3.0000e+01, 6.1450e-01, 3.0000e+00],\n",
       "        [9.2360e+02, 4.4683e+02, 9.5630e+02, 5.0179e+02, 4.2000e+01, 5.9320e-01, 3.0000e+00],\n",
       "        [8.3729e+02, 4.2984e+02, 8.6565e+02, 4.7382e+02, 9.8000e+01, 5.9259e-01, 3.0000e+00],\n",
       "        [8.0808e+02, 2.9993e+02, 8.2580e+02, 3.3520e+02, 1.2000e+01, 4.7062e-01, 0.0000e+00],\n",
       "        [1.2323e+03, 3.3045e+02, 1.2797e+03, 3.8416e+02, 1.2000e+02, 8.1126e-01, 2.0000e+00],\n",
       "        [7.7994e+02, 3.4836e+02, 7.9886e+02, 3.8032e+02, 1.4700e+02, 5.2082e-01, 3.0000e+00],\n",
       "        [8.2206e+02, 2.2383e+02, 8.5279e+02, 2.4928e+02, 1.5700e+02, 4.1791e-01, 2.0000e+00],\n",
       "        [7.9926e+02, 3.2958e+02, 8.1605e+02, 3.6255e+02, 2.2000e+01, 3.5703e-01, 3.0000e+00],\n",
       "        [8.3567e+02, 4.1066e+02, 8.6584e+02, 4.5546e+02, 1.5000e+01, 2.5076e-01, 0.0000e+00],\n",
       "        [8.2366e+02, 3.5048e+02, 8.4661e+02, 3.9199e+02, 1.3000e+01, 3.6298e-01, 0.0000e+00],\n",
       "        [1.2310e+03, 2.7352e+02, 1.2782e+03, 3.1123e+02, 1.8900e+02, 6.0139e-01, 2.0000e+00],\n",
       "        [8.7397e+02, 2.4153e+02, 9.0291e+02, 2.7313e+02, 1.6100e+02, 3.3176e-01, 2.0000e+00],\n",
       "        [8.7948e+02, 2.5992e+02, 9.1711e+02, 2.9348e+02, 1.0000e+01, 6.3040e-01, 2.0000e+00],\n",
       "        [8.7493e+02, 3.7039e+02, 8.9952e+02, 4.0666e+02, 1.1800e+02, 3.7091e-01, 0.0000e+00],\n",
       "        [7.2562e+02, 2.0736e+02, 7.8538e+02, 2.8251e+02, 2.0000e+02, 6.2098e-01, 5.0000e+00],\n",
       "        [9.2619e+02, 4.3171e+02, 9.5469e+02, 4.7093e+02, 2.0300e+02, 4.7045e-01, 0.0000e+00],\n",
       "        [8.2567e+02, 3.7041e+02, 8.4648e+02, 4.0329e+02, 2.1900e+02, 5.6395e-01, 3.0000e+00],\n",
       "        [7.7781e+02, 3.3348e+02, 7.9723e+02, 3.6962e+02, 2.2500e+02, 3.7817e-01, 0.0000e+00],\n",
       "        [7.7797e+02, 4.3654e+02, 8.1221e+02, 4.8237e+02, 1.4400e+02, 3.9165e-01, 0.0000e+00],\n",
       "        [8.4405e+02, 2.0868e+02, 8.7088e+02, 2.3352e+02, 2.3700e+02, 2.9431e-01, 2.0000e+00]])\n",
       "id: tensor([  1.,   2.,   4.,   5.,   6.,   8.,   9.,  20.,  30.,  42.,  98.,  12., 120., 147., 157.,  22.,  15.,  13., 189., 161.,  10., 118., 200., 203., 219., 225., 144., 237.])\n",
       "is_track: True\n",
       "orig_shape: (720, 1280)\n",
       "shape: torch.Size([28, 7])\n",
       "xywh: tensor([[ 220.0146,  562.5936,  438.8896,  223.7467],\n",
       "        [1143.6792,  304.9027,   59.4374,   40.2373],\n",
       "        [1179.3633,  287.2715,   51.6921,   33.8652],\n",
       "        [ 771.9658,  243.7910,   91.8877,   76.8888],\n",
       "        [ 794.5943,  480.6381,   33.8585,   51.6685],\n",
       "        [ 841.8025,  260.5541,   34.2199,   29.4645],\n",
       "        [ 888.4366,  231.8747,   26.9761,   25.6726],\n",
       "        [ 859.8578,  340.4581,   19.6675,   37.0900],\n",
       "        [ 888.0524,  408.6633,   23.5405,   38.5580],\n",
       "        [ 939.9513,  474.3140,   32.7031,   54.9581],\n",
       "        [ 851.4717,  451.8322,   28.3649,   43.9813],\n",
       "        [ 816.9408,  317.5673,   17.7178,   35.2655],\n",
       "        [1255.9736,  357.3008,   47.4160,   53.7105],\n",
       "        [ 789.4018,  364.3439,   18.9194,   31.9614],\n",
       "        [ 837.4247,  236.5571,   30.7354,   25.4507],\n",
       "        [ 807.6537,  346.0681,   16.7938,   32.9738],\n",
       "        [ 850.7516,  433.0594,   30.1728,   44.8009],\n",
       "        [ 835.1328,  371.2343,   22.9481,   41.5126],\n",
       "        [1254.6072,  292.3764,   47.2694,   37.7063],\n",
       "        [ 888.4409,  257.3293,   28.9355,   31.5990],\n",
       "        [ 898.2935,  276.6999,   37.6350,   33.5605],\n",
       "        [ 887.2271,  388.5226,   24.5853,   36.2740],\n",
       "        [ 755.4972,  244.9358,   59.7639,   75.1424],\n",
       "        [ 940.4377,  451.3237,   28.4988,   39.2195],\n",
       "        [ 836.0740,  386.8459,   20.8087,   32.8807],\n",
       "        [ 787.5227,  351.5490,   19.4214,   36.1368],\n",
       "        [ 795.0908,  459.4545,   34.2405,   45.8295],\n",
       "        [ 857.4644,  221.0997,   26.8341,   24.8311]])\n",
       "xywhn: tensor([[0.1719, 0.7814, 0.3429, 0.3108],\n",
       "        [0.8935, 0.4235, 0.0464, 0.0559],\n",
       "        [0.9214, 0.3990, 0.0404, 0.0470],\n",
       "        [0.6031, 0.3386, 0.0718, 0.1068],\n",
       "        [0.6208, 0.6676, 0.0265, 0.0718],\n",
       "        [0.6577, 0.3619, 0.0267, 0.0409],\n",
       "        [0.6941, 0.3220, 0.0211, 0.0357],\n",
       "        [0.6718, 0.4729, 0.0154, 0.0515],\n",
       "        [0.6938, 0.5676, 0.0184, 0.0536],\n",
       "        [0.7343, 0.6588, 0.0255, 0.0763],\n",
       "        [0.6652, 0.6275, 0.0222, 0.0611],\n",
       "        [0.6382, 0.4411, 0.0138, 0.0490],\n",
       "        [0.9812, 0.4963, 0.0370, 0.0746],\n",
       "        [0.6167, 0.5060, 0.0148, 0.0444],\n",
       "        [0.6542, 0.3286, 0.0240, 0.0353],\n",
       "        [0.6310, 0.4807, 0.0131, 0.0458],\n",
       "        [0.6646, 0.6015, 0.0236, 0.0622],\n",
       "        [0.6524, 0.5156, 0.0179, 0.0577],\n",
       "        [0.9802, 0.4061, 0.0369, 0.0524],\n",
       "        [0.6941, 0.3574, 0.0226, 0.0439],\n",
       "        [0.7018, 0.3843, 0.0294, 0.0466],\n",
       "        [0.6931, 0.5396, 0.0192, 0.0504],\n",
       "        [0.5902, 0.3402, 0.0467, 0.1044],\n",
       "        [0.7347, 0.6268, 0.0223, 0.0545],\n",
       "        [0.6532, 0.5373, 0.0163, 0.0457],\n",
       "        [0.6153, 0.4883, 0.0152, 0.0502],\n",
       "        [0.6212, 0.6381, 0.0268, 0.0637],\n",
       "        [0.6699, 0.3071, 0.0210, 0.0345]])\n",
       "xyxy: tensor([[5.6978e-01, 4.5072e+02, 4.3946e+02, 6.7447e+02],\n",
       "        [1.1140e+03, 2.8478e+02, 1.1734e+03, 3.2502e+02],\n",
       "        [1.1535e+03, 2.7034e+02, 1.2052e+03, 3.0420e+02],\n",
       "        [7.2602e+02, 2.0535e+02, 8.1791e+02, 2.8224e+02],\n",
       "        [7.7767e+02, 4.5480e+02, 8.1152e+02, 5.0647e+02],\n",
       "        [8.2469e+02, 2.4582e+02, 8.5891e+02, 2.7529e+02],\n",
       "        [8.7495e+02, 2.1904e+02, 9.0192e+02, 2.4471e+02],\n",
       "        [8.5002e+02, 3.2191e+02, 8.6969e+02, 3.5900e+02],\n",
       "        [8.7628e+02, 3.8938e+02, 8.9982e+02, 4.2794e+02],\n",
       "        [9.2360e+02, 4.4683e+02, 9.5630e+02, 5.0179e+02],\n",
       "        [8.3729e+02, 4.2984e+02, 8.6565e+02, 4.7382e+02],\n",
       "        [8.0808e+02, 2.9993e+02, 8.2580e+02, 3.3520e+02],\n",
       "        [1.2323e+03, 3.3045e+02, 1.2797e+03, 3.8416e+02],\n",
       "        [7.7994e+02, 3.4836e+02, 7.9886e+02, 3.8032e+02],\n",
       "        [8.2206e+02, 2.2383e+02, 8.5279e+02, 2.4928e+02],\n",
       "        [7.9926e+02, 3.2958e+02, 8.1605e+02, 3.6255e+02],\n",
       "        [8.3567e+02, 4.1066e+02, 8.6584e+02, 4.5546e+02],\n",
       "        [8.2366e+02, 3.5048e+02, 8.4661e+02, 3.9199e+02],\n",
       "        [1.2310e+03, 2.7352e+02, 1.2782e+03, 3.1123e+02],\n",
       "        [8.7397e+02, 2.4153e+02, 9.0291e+02, 2.7313e+02],\n",
       "        [8.7948e+02, 2.5992e+02, 9.1711e+02, 2.9348e+02],\n",
       "        [8.7493e+02, 3.7039e+02, 8.9952e+02, 4.0666e+02],\n",
       "        [7.2562e+02, 2.0736e+02, 7.8538e+02, 2.8251e+02],\n",
       "        [9.2619e+02, 4.3171e+02, 9.5469e+02, 4.7093e+02],\n",
       "        [8.2567e+02, 3.7041e+02, 8.4648e+02, 4.0329e+02],\n",
       "        [7.7781e+02, 3.3348e+02, 7.9723e+02, 3.6962e+02],\n",
       "        [7.7797e+02, 4.3654e+02, 8.1221e+02, 4.8237e+02],\n",
       "        [8.4405e+02, 2.0868e+02, 8.7088e+02, 2.3352e+02]])\n",
       "xyxyn: tensor([[4.4514e-04, 6.2600e-01, 3.4333e-01, 9.3676e-01],\n",
       "        [8.7028e-01, 3.9553e-01, 9.1672e-01, 4.5142e-01],\n",
       "        [9.0119e-01, 3.7547e-01, 9.4157e-01, 4.2251e-01],\n",
       "        [5.6720e-01, 2.8520e-01, 6.3899e-01, 3.9199e-01],\n",
       "        [6.0755e-01, 6.3167e-01, 6.3400e-01, 7.0343e-01],\n",
       "        [6.4429e-01, 3.4142e-01, 6.7103e-01, 3.8234e-01],\n",
       "        [6.8355e-01, 3.0422e-01, 7.0463e-01, 3.3988e-01],\n",
       "        [6.6408e-01, 4.4710e-01, 6.7945e-01, 4.9862e-01],\n",
       "        [6.8460e-01, 5.4081e-01, 7.0299e-01, 5.9436e-01],\n",
       "        [7.2156e-01, 6.2060e-01, 7.4711e-01, 6.9693e-01],\n",
       "        [6.5413e-01, 5.9700e-01, 6.7629e-01, 6.5809e-01],\n",
       "        [6.3131e-01, 4.1658e-01, 6.4516e-01, 4.6556e-01],\n",
       "        [9.6271e-01, 4.5895e-01, 9.9975e-01, 5.3355e-01],\n",
       "        [6.0933e-01, 4.8384e-01, 6.2411e-01, 5.2823e-01],\n",
       "        [6.4223e-01, 3.1088e-01, 6.6624e-01, 3.4623e-01],\n",
       "        [6.2442e-01, 4.5775e-01, 6.3754e-01, 5.0355e-01],\n",
       "        [6.5286e-01, 5.7036e-01, 6.7644e-01, 6.3258e-01],\n",
       "        [6.4348e-01, 4.8677e-01, 6.6141e-01, 5.4443e-01],\n",
       "        [9.6170e-01, 3.7989e-01, 9.9863e-01, 4.3226e-01],\n",
       "        [6.8279e-01, 3.3546e-01, 7.0540e-01, 3.7935e-01],\n",
       "        [6.8709e-01, 3.6100e-01, 7.1649e-01, 4.0761e-01],\n",
       "        [6.8354e-01, 5.1442e-01, 7.0275e-01, 5.6481e-01],\n",
       "        [5.6689e-01, 2.8801e-01, 6.1358e-01, 3.9237e-01],\n",
       "        [7.2358e-01, 5.9960e-01, 7.4585e-01, 6.5407e-01],\n",
       "        [6.4505e-01, 5.1445e-01, 6.6131e-01, 5.6012e-01],\n",
       "        [6.0767e-01, 4.6317e-01, 6.2284e-01, 5.1336e-01],\n",
       "        [6.0779e-01, 6.0631e-01, 6.3454e-01, 6.6996e-01],\n",
       "        [6.5941e-01, 2.8984e-01, 6.8038e-01, 3.2433e-01]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7236b9c-0c8c-4ad9-98c6-824842c22c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 10 cars, 3 motorcycles, 3 buss, 1 truck, 837.7ms\n",
      "Speed: 4.2ms preprocess, 837.7ms inference, 46.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 3 motorcycles, 3 buss, 1 truck, 468.0ms\n",
      "Speed: 3.1ms preprocess, 468.0ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 3 motorcycles, 3 buss, 1 truck, 426.0ms\n",
      "Speed: 2.3ms preprocess, 426.0ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 3 motorcycles, 3 buss, 1 truck, 369.8ms\n",
      "Speed: 2.4ms preprocess, 369.8ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 4 motorcycles, 3 buss, 1 truck, 385.0ms\n",
      "Speed: 2.8ms preprocess, 385.0ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 4 motorcycles, 3 buss, 1 truck, 440.8ms\n",
      "Speed: 2.5ms preprocess, 440.8ms inference, 7.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 4 motorcycles, 3 buss, 2 trucks, 404.9ms\n",
      "Speed: 2.7ms preprocess, 404.9ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 4 motorcycles, 3 buss, 2 trucks, 357.4ms\n",
      "Speed: 2.8ms preprocess, 357.4ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 3 motorcycles, 3 buss, 2 trucks, 355.4ms\n",
      "Speed: 3.1ms preprocess, 355.4ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 3 motorcycles, 3 buss, 1 truck, 371.2ms\n",
      "Speed: 2.2ms preprocess, 371.2ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 4 motorcycles, 3 buss, 1 truck, 354.0ms\n",
      "Speed: 1.9ms preprocess, 354.0ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 3 motorcycles, 3 buss, 2 trucks, 415.5ms\n",
      "Speed: 2.6ms preprocess, 415.5ms inference, 7.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 5 motorcycles, 3 buss, 1 truck, 362.6ms\n",
      "Speed: 2.1ms preprocess, 362.6ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 5 motorcycles, 3 buss, 1 truck, 351.0ms\n",
      "Speed: 1.8ms preprocess, 351.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 5 motorcycles, 3 buss, 1 truck, 338.9ms\n",
      "Speed: 2.5ms preprocess, 338.9ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 5 motorcycles, 3 buss, 1 truck, 350.3ms\n",
      "Speed: 2.2ms preprocess, 350.3ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 5 motorcycles, 3 buss, 1 truck, 408.8ms\n",
      "Speed: 2.5ms preprocess, 408.8ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 6 motorcycles, 3 buss, 1 truck, 388.6ms\n",
      "Speed: 2.5ms preprocess, 388.6ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 6 motorcycles, 3 buss, 1 truck, 352.0ms\n",
      "Speed: 2.7ms preprocess, 352.0ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 6 motorcycles, 3 buss, 1 truck, 336.0ms\n",
      "Speed: 2.5ms preprocess, 336.0ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 5 motorcycles, 3 buss, 1 truck, 342.6ms\n",
      "Speed: 2.0ms preprocess, 342.6ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 6 motorcycles, 3 buss, 1 truck, 337.8ms\n",
      "Speed: 2.3ms preprocess, 337.8ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 6 motorcycles, 3 buss, 1 truck, 340.3ms\n",
      "Speed: 2.0ms preprocess, 340.3ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 5 motorcycles, 3 buss, 1 truck, 341.8ms\n",
      "Speed: 1.8ms preprocess, 341.8ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 5 motorcycles, 3 buss, 372.6ms\n",
      "Speed: 1.9ms preprocess, 372.6ms inference, 7.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 6 motorcycles, 3 buss, 350.5ms\n",
      "Speed: 1.8ms preprocess, 350.5ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 5 motorcycles, 3 buss, 333.7ms\n",
      "Speed: 1.8ms preprocess, 333.7ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 5 motorcycles, 3 buss, 1 truck, 348.3ms\n",
      "Speed: 2.1ms preprocess, 348.3ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 5 motorcycles, 3 buss, 1 truck, 446.2ms\n",
      "Speed: 1.9ms preprocess, 446.2ms inference, 8.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 5 motorcycles, 3 buss, 1 truck, 505.8ms\n",
      "Speed: 2.5ms preprocess, 505.8ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 5 motorcycles, 3 buss, 466.0ms\n",
      "Speed: 3.5ms preprocess, 466.0ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 5 motorcycles, 3 buss, 463.9ms\n",
      "Speed: 2.8ms preprocess, 463.9ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 5 motorcycles, 3 buss, 467.3ms\n",
      "Speed: 2.8ms preprocess, 467.3ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 5 motorcycles, 3 buss, 499.5ms\n",
      "Speed: 40.5ms preprocess, 499.5ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 5 motorcycles, 3 buss, 468.7ms\n",
      "Speed: 3.2ms preprocess, 468.7ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 4 motorcycles, 3 buss, 512.3ms\n",
      "Speed: 2.9ms preprocess, 512.3ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 4 motorcycles, 3 buss, 475.7ms\n",
      "Speed: 2.6ms preprocess, 475.7ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 4 motorcycles, 3 buss, 459.5ms\n",
      "Speed: 2.5ms preprocess, 459.5ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 4 motorcycles, 4 buss, 462.0ms\n",
      "Speed: 2.5ms preprocess, 462.0ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 5 motorcycles, 4 buss, 464.1ms\n",
      "Speed: 2.6ms preprocess, 464.1ms inference, 7.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 6 motorcycles, 4 buss, 470.7ms\n",
      "Speed: 2.6ms preprocess, 470.7ms inference, 7.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 6 motorcycles, 4 buss, 482.2ms\n",
      "Speed: 2.9ms preprocess, 482.2ms inference, 7.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 6 motorcycles, 4 buss, 461.5ms\n",
      "Speed: 2.6ms preprocess, 461.5ms inference, 7.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 6 motorcycles, 4 buss, 483.7ms\n",
      "Speed: 2.7ms preprocess, 483.7ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 6 motorcycles, 4 buss, 715.4ms\n",
      "Speed: 2.4ms preprocess, 715.4ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 6 motorcycles, 4 buss, 473.3ms\n",
      "Speed: 2.9ms preprocess, 473.3ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 6 motorcycles, 4 buss, 471.9ms\n",
      "Speed: 2.5ms preprocess, 471.9ms inference, 8.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 6 motorcycles, 4 buss, 491.3ms\n",
      "Speed: 3.0ms preprocess, 491.3ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 6 motorcycles, 4 buss, 464.5ms\n",
      "Speed: 2.7ms preprocess, 464.5ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 6 motorcycles, 4 buss, 443.1ms\n",
      "Speed: 2.7ms preprocess, 443.1ms inference, 7.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 6 motorcycles, 3 buss, 454.9ms\n",
      "Speed: 2.5ms preprocess, 454.9ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 6 motorcycles, 2 buss, 438.6ms\n",
      "Speed: 2.8ms preprocess, 438.6ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 6 motorcycles, 2 buss, 419.3ms\n",
      "Speed: 2.6ms preprocess, 419.3ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 6 motorcycles, 2 buss, 366.0ms\n",
      "Speed: 2.1ms preprocess, 366.0ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 6 motorcycles, 2 buss, 365.3ms\n",
      "Speed: 2.0ms preprocess, 365.3ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 5 motorcycles, 2 buss, 410.3ms\n",
      "Speed: 2.2ms preprocess, 410.3ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 5 motorcycles, 3 buss, 443.3ms\n",
      "Speed: 2.3ms preprocess, 443.3ms inference, 8.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 5 motorcycles, 3 buss, 450.4ms\n",
      "Speed: 2.5ms preprocess, 450.4ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 6 motorcycles, 3 buss, 1 truck, 444.4ms\n",
      "Speed: 2.4ms preprocess, 444.4ms inference, 9.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 6 motorcycles, 2 buss, 1 truck, 417.7ms\n",
      "Speed: 3.4ms preprocess, 417.7ms inference, 7.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 6 motorcycles, 3 buss, 1 truck, 403.7ms\n",
      "Speed: 3.1ms preprocess, 403.7ms inference, 7.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 7 motorcycles, 3 buss, 1 truck, 412.6ms\n",
      "Speed: 2.7ms preprocess, 412.6ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 7 motorcycles, 2 buss, 1 truck, 458.4ms\n",
      "Speed: 2.5ms preprocess, 458.4ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# Load the YOLO model\n",
    "model = YOLO('yolo11l.pt')\n",
    "\n",
    "class_list = model.names \n",
    "#class_list\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture('test_videos/clip.mp4')\n",
    "\n",
    "line_y_red = 430  # Red line position\n",
    "\n",
    "# Dictionary to store object counts by class\n",
    "class_counts = defaultdict(int)\n",
    "\n",
    "# Dictionary to keep track of object IDs that have crossed the line\n",
    "crossed_ids = set()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read() # Reads each frame from the video\n",
    "    if not ret: # If no frame to read again\n",
    "        break\n",
    "\n",
    "    # Run YOLO tracking on the frames one after the other\n",
    "    results = model.track(frame, persist = True, classes = [1, 2, 3, 5, 6, 7])\n",
    "    # print(results)\n",
    "\n",
    "    if results[0].boxes.data is not None:\n",
    "        # Get the detected boxes, their class indices, track_ids\n",
    "        boxes = results[0].boxes.xyxy.cpu()\n",
    "        track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
    "        confidences = results[0].boxes.conf.cpu()\n",
    "\n",
    "        cv2.line(frame, (690, line_y_red), (1130, line_y_red), (0, 0, 255), 3)\n",
    "        #cv2.putText(frame, 'Red Line', (690, line_y_red - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    # Loop through each detected object\n",
    "    for box, track_id, class_idx, conf in zip(boxes, track_ids, class_indices, confidences):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cx = (x1 + x2) // 2  # Calculate the center point\n",
    "        cy = (y1 + y2) // 2            \n",
    "\n",
    "        class_name = class_list[class_idx]\n",
    "\n",
    "        cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
    "        \n",
    "        cv2.putText(frame, f\"ID: {track_id} {class_name}\", (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # Check if the object has crossed the red line\n",
    "        if cy > line_y_red and track_id not in crossed_ids:\n",
    "            # Mark the object as crossed\n",
    "            crossed_ids.add(track_id)\n",
    "            class_counts[class_name] += 1\n",
    "\n",
    "    # Display the counts on the frame\n",
    "    y_offset = 30\n",
    "    for class_name, count in class_counts.items():\n",
    "        cv2.putText(frame, f\"{class_name}: {count}\", (50, y_offset),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "        y_offset += 30\n",
    "        \n",
    "    \n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Yolo Object tracking and counting\", frame)\n",
    "\n",
    "    # Exit loop id 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d1b8f-2ee8-4d68-95f0-640929be39fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (yolov11)",
   "language": "python",
   "name": "yolov11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
